{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "796844c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eliot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eliot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\eliot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eliot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from torchtext.vocab import GloVe\n",
    "import torchtext.vocab \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e9a11",
   "metadata": {},
   "source": [
    "# Eliot's Interactive Syllabus Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d026c3",
   "metadata": {},
   "source": [
    "Code inspired by tutorial created by [Patrick Loeber](https://www.youtube.com/playlist?list=PLqnslRFeH2UrFW4AUgn-eY37qOAWQpJyg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90103d4f",
   "metadata": {},
   "source": [
    "### Preparing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ab370",
   "metadata": {},
   "source": [
    "The code in the cell below defines two functions to preprocess text data and create a bag of words representation for a given sentence.\n",
    "\n",
    "The preprocess function takes a sentence as input, removes punctuation and stop words, stems each word in the sentence, and returns the preprocessed sentence as a list of words.\n",
    "\n",
    "The bag_of_words function takes a tokenized sentence and a list of all known words in the vocabulary as input, and creates a bag of words representation for the given sentence. It initializes the bag with zeros for each word in the vocabulary, and updates the bag with 1 for each word in the sentence that exists in the vocabulary. The function returns a numpy array representing the bag of words with 1 for each known word that exists in the sentence, 0 otherwise.\n",
    "\n",
    "The techniques in used in these functions are commonly used in natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369c0bf",
   "metadata": {},
   "source": [
    "NOTE: the stop words list provided by NLTK includes words that I don't think are stop words, and it also doesn't include some words that I do think are stop words for the purposes of this project. Thus, I start by modifying the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c20a42b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "'do' in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57c700b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('where')\n",
    "stop_words.remove('when')\n",
    "stop_words.remove('what')\n",
    "stop_words.remove('who')\n",
    "stop_words.remove('how')\n",
    "stop_words.remove('more')\n",
    "stop_words.add('tell')\n",
    "stop_words.add('know')\n",
    "stop_words.add('cs')\n",
    "stop_words.add('cis')\n",
    "stop_words.add('academic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a904335b",
   "metadata": {},
   "source": [
    "Next, let's create a dictionary of meaningful synonyms. By replacing all synonyms with a single word, we can reduce the number of features in our feature space while increasing the frequency of important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ba9a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {\n",
    "    'location': {'room', 'classroom', 'building', 'place', 'site', 'hall'},\n",
    "    'disability': {'injury', 'disorder', 'condition', 'impairment', 'impaired',\n",
    "                  'injured', 'ptsd', 'anxiety', 'dyslexia', 'adhd', 'depression',\n",
    "                  'illness', 'disease', 'autism'},\n",
    "    'accomadate': {'support', 'supported', 'supporting', 'supports',\n",
    "             'accomadate', 'accomadates', 'accommodation', 'accomadated' \n",
    "             'assistance', 'assist', 'assisting', 'assists'},\n",
    "    'more': {'added', 'additional', 'extra'},\n",
    "    'zoom': {'remote'},\n",
    "    'ml': {'ai', 'rnn', 'cnn', 'PCA', 'dimensionality', 'regression', 'knn', \n",
    "           'svm', 'svms', 'gans', 'gan', 'optimization', 'cnns', 'transformers',\n",
    "          'vaes', 'vae', 'regularization', 'gradient', 'neural', 'network'},\n",
    "    'test': {'exam', 'test', 'midterm'},\n",
    "    'dishonesty': {'dishonesty', 'cheating', 'plagiarize', 'copying', 'copied',\n",
    "                   'collusion', 'plagiarism', 'lying', 'cheat'},\n",
    "    '315': {'315', 'cs315', 'cis315', 'cs415', '415', '313', 'cs313', 'cis313', \n",
    "           'cis415', 'cs314', '314', 'cis314', 'cs212', 'cis212', '212',\n",
    "           'cs211', 'cis211', '211', 'cs210', 'cis210', '210',\n",
    "           'cs425', 'cis425', '425', 'cs471', 'cis471', '471', \n",
    "           'cs330', 'cis330', '330'},\n",
    "    'course': ['cs472', 'class', 'course', '472', 'cis472', 'curriculum', 'lecture',\n",
    "              'lectures'],\n",
    "    'cs': {'cis', 'cs'},\n",
    "    'you': {'ya', 'you'},\n",
    "    'time': {'monday', 'tuesday', 'wednesday', \n",
    "             'thursday', 'friday','saturday', 'sunday',\n",
    "             'tomorrow', 'yesterday', 'today', 'january', 'february', \n",
    "             'march', 'april', 'may', 'june', 'july', 'august',\n",
    "             'september', 'october', 'november', 'december',\n",
    "             'tonight', 'afternoon', 'tonight', 'morning'},\n",
    "    'software': {'pytorch', 'tensorflow', 'numpy', 'pandas', \n",
    "               'language', 'framework', 'javascript', 'c',\n",
    "              'sklearn', 'scikitlearn', 'keras', 'jupyter', 'python'},\n",
    "    'coding': {'programming', 'code', 'program', 'develop', 'developing'},\n",
    "    'start': ['begin', 'commence'],\n",
    "    'computer': {'pc', 'mac', 'laptop'},\n",
    "    'employee': {'employee', 'worker', 'staff'},\n",
    "    'ta': {'ge', 'steven', 'walton'},\n",
    "    'good': {'great', 'nice', 'awesome', 'cool'},\n",
    "    'goodbye': {'adieu', 'farewell', \n",
    "                'bye', 'adios', 'arrivederci', \n",
    "                'auf', 'ciao', 'later', 'peace', \n",
    "                'sayonara', 'see ya', 'ttyl', \n",
    "                'wiedersehen'},\n",
    "    'grad': {'graduate'},\n",
    "    'hi': {'hello', 'hey', 'hiya', 'hola', 'greetings', 'yo'},\n",
    "    'i': {'myself'},\n",
    "    'long': {'lengthy'},\n",
    "    'prerequisites': {'prereqs', 'prerequisite'},\n",
    "    'professor': {'teacher', 'instructor', 'prof', 'humphrey', 'shi'}\n",
    "}\n",
    "\n",
    "def synonymReplacer(tokens, stemmed=False):\n",
    "    # loop through each token in the list\n",
    "    for i, word in enumerate(tokens):\n",
    "        # convert the word to lowercase\n",
    "        word = word.lower()\n",
    "        # loop through each synonym list in the synonyms dictionary\n",
    "        for syn, syn_list in synonyms.items():\n",
    "            # check if the current word is in the synonym list\n",
    "            if stemmed:\n",
    "                syn_list = [stemmer.stem(w) for w in syn_list]\n",
    "            if word in syn_list:\n",
    "                # replace the current token with the synonym\n",
    "                tokens[i] = syn\n",
    "                # break out of the loop since we've found a synonym\n",
    "                break\n",
    "    # return the modified list of tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7bb7f140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when', 'does', 'course', 'start']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonymReplacer([\"when\", \"does\", \"class\", \"begin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9b38b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a sentence as input and performs various text preprocessing steps on it,\n",
    "    including removing punctuation, stop words, and stemming each word in the sentence.\n",
    "    \"\"\"\n",
    "    # remove punctuation from sentence\n",
    "    sentence = ''.join(\n",
    "        char for char in sentence if char not in string.punctuation\n",
    "    )\n",
    "    # tokenizing the sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # replace synonyms\n",
    "    tokens = synonymReplacer(tokens)\n",
    "    # removing stop words\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    # stemming each word in the sentence\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    stemmed_words = synonymReplacer(stemmed_words, stemmed=True)\n",
    "    # return the preprocessed sentence as a list of words\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "\n",
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    Create a bag of words representation for a given tokenized sentence.\n",
    "    \"\"\"\n",
    "    # initialize the bag with zeros for each word in the vocabulary\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "\n",
    "    # update the bag with 1 for each word in the sentence that exists in the vocabulary\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in tokenized_sentence: \n",
    "            bag[idx] = 1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1242c",
   "metadata": {},
   "source": [
    "#### Preprocessing Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c766d",
   "metadata": {},
   "source": [
    "What exactly does our preprocessing do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a497c33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who', 'lead', 'course', 'course']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"Can you tell me who will be leading the lectures for CS 472?\"\n",
    "tokens = preprocess(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f296c5d",
   "metadata": {},
   "source": [
    "First, the `preprocess` function removes all punctuation marks from the sentence using the `string.punctuation` module. Then, the sentence is tokenized into a list of words using the nltk.word_tokenize method.\n",
    "\n",
    "Next, the function removes stop words, which are common words that do not carry much meaning in the sentence, such as \"a\", \"an\", \"the\", \"of\", and so on. In this case, the function is using a pre-defined list of stop words to remove them from the list of tokens.\n",
    "\n",
    "After that, the function performs stemming on each word in the sentence, which involves converting the words into their root or base form, called their stem. The function uses a stemmer to perform this task.\n",
    "\n",
    "Finally, the preprocessed words are returned as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67a32bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_all_words = ['hello', 'tell', 'what', 'ten', 'lead', 'lectur', 'cs', 'who', '472']\n",
    "bag_of_words(tokens, example_all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d089fcb",
   "metadata": {},
   "source": [
    "The purpose of the code in the below cell is to read in the data file, tokenize the sentences into individual words, and create a list of (X, y) pairs, where X is a list of tokenized words and y is the associated intent tag. This is a common preprocessing step in Natural Language Processing (NLP) where the goal is to classify user input into one of several predefined categories. By tokenizing the input patterns and creating (X, y) pairs, the data can be transformed into a format that is more suitable for use in machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37011752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data file as a Python object\n",
    "with open('intents.json', 'r') as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# empty lists and dictionary for storing the tokenized words, tags, and (X, y) pairs\n",
    "all_words = []\n",
    "tags = []\n",
    "xy_pairs = []\n",
    "bert_pairs = []\n",
    "word_counts = {}\n",
    "\n",
    "# loop through each intent in the data file\n",
    "for intent in intents['intents']:\n",
    "    # get the tag\n",
    "    tag = intent['tag']\n",
    "    # add the tag to our list of tags\n",
    "    tags.append(tag)\n",
    "    # loop through each pattern (sentence) in the intent\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize the pattern into a list of words\n",
    "        words = preprocess(pattern)\n",
    "        # loop through each word in the tokenized pattern\n",
    "        for word in words:\n",
    "            # add the word to our list of all words\n",
    "            all_words.append(word)\n",
    "            # add the word to our word_counts dictionary and increment its count\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 1\n",
    "            else:\n",
    "                word_counts[word] += 1\n",
    "        # add the (words, tag) pair to our list of (X, y) pairs\n",
    "        xy_pairs.append((words, tag))\n",
    "        bert_pairs.append((pattern, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa11cf9",
   "metadata": {},
   "source": [
    "The purpose of the code is to preprocess a list of words and associated intent tags for use in Natural Language Processing (NLP) tasks such as intent classification. The code first stems and lowercases each word in the list, ignoring any words in a specified ignore list. It then removes duplicate words from the list and sorts the resulting list. The code also sorts the list of intent tags. The resulting processed data can be used as input to machine learning algorithms for tasks such as training a model to classify user input into one of several predefined categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56014206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate words and sort the list\n",
    "all_words = sorted(set(all_words))\n",
    "\n",
    "# sort the list of tags\n",
    "tags = sorted(set(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219782c5",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661f3b4",
   "metadata": {},
   "source": [
    "The code in the cell below creates the training data by converting each input sentence in the original data to a bag of words representation using the bag_of_words function, and then converting the intent tags to integer labels that can be used for training. The resulting training data is represented as two numpy arrays, X_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5333d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data1 = []\n",
    "y_data1 = []\n",
    "\n",
    "# for each (X, y) pair in the data\n",
    "for (pattern_sentence, tag) in xy_pairs:\n",
    "    # create a bag of words for the pattern sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    # append the bag of words to the X data\n",
    "    X_data1.append(bag)\n",
    "    # convert the tag to a label that can be used with PyTorch CrossEntropyLoss\n",
    "    label = tags.index(tag)\n",
    "    # append the label to the y data\n",
    "    y_data1.append(label)\n",
    "\n",
    "# convert X and y to numpy arrays\n",
    "X_data1 = np.array(X_data1)\n",
    "y_data1 = np.array(y_data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3ccf1",
   "metadata": {},
   "source": [
    "Now we can get training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2a6b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into 10% test set and 90% train set\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_data1, y_data1, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18e965",
   "metadata": {},
   "source": [
    "### Create model to classify text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf5812",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4eb6010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503 16\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "num_epochs = 500\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 32\n",
    "output_size = len(tags)\n",
    "max_seq_length = 50\n",
    "print(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e022bc",
   "metadata": {},
   "source": [
    "In order to effectively capture the underlying patterns in language data, a model capable of learning complex relationships is required. However, it is important to strike a balance and avoid overfitting by not making the model more complex than necessary. To start with, a simple feed forward neural network was chosen.\n",
    "\n",
    "The model presented in the cell below uses three fully connected layers, providing the capability to learn complex patterns in the input data. The ReLU activation function between the first and second fully connected layers introduces non-linearity into the model, improving its ability to model complex relationships. The third fully connected layer maps the learned features to the output classes, and no activation function is used for this layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab67f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(BasicNN, self).__init__()\n",
    "        # the first fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # the second fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) \n",
    "        # the third fully connected layer\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        # the ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # zero out certain values to help prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # apply the ReLU activation function to the first fully connected layer\n",
    "        out = self.relu(self.fc1(x))  # apply relu to the 1st fully connected layer output\n",
    "        # apply dropout regularization to prevent overfitting on the 2nd fully connected layer\n",
    "        out = self.dropout(out)\n",
    "        # apply the ReLU activation function to the second fully connected layer\n",
    "        out = self.relu(self.fc2(out))  # apply relu to the 2nd fully connected layer output\n",
    "        # apply dropout regularization to prevent overfitting on the 3rd fully connected layer\n",
    "        out = self.dropout(out)\n",
    "        # apply the third fully connected layer without any activation function\n",
    "        out = self.fc3(out)  # apply the last fully connected layer\n",
    "        return out  # return the final output of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c668e5",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821aed6",
   "metadata": {},
   "source": [
    "The `ChatDataset` class is a PyTorch dataset object that is designed to be used with PyTorch's DataLoader module to retrieve training data and corresponding labels for my chatbot application. The purpose of this class is to encapsulate the training data and labels as attributes of the object and to define the methods `__getitem__()` and `__len__()` to support indexing and length operations, respectively, on the dataset object. This class is an implementation of the Dataset abstract class in PyTorch and provides a consistent interface for loading training data for use with PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3415550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset object that is designed to be used with DataLoader to retrieve\n",
    "    training data and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Initializes the ChatDataset object.\n",
    "        \"\"\"\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns the specified training sample.\n",
    "        \"\"\"\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of training samples.\n",
    "        \"\"\"\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7268998",
   "metadata": {},
   "source": [
    "This code creates a `ChatDataset` object with training data and corresponding labels `X_train` and `y_train`, and then uses a PyTorch DataLoader object to iterate over the dataset during model training. The `DataLoader` is configured to retrieve data in batches of size `batch_size`, to shuffle the data before each epoch of training, and to use 0 worker processes for data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9ec14acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset1 = ChatDataset(X_train1, y_train1)\n",
    "train_loader1 = DataLoader(dataset=training_dataset1,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d91d43",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f657555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, num_epochs, train_loader, bert=False):\n",
    "    # Move the loss function to GPU\n",
    "    criterion = criterion.to(device)\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs): # loop over the specified number of epochs\n",
    "        for (words, labels) in train_loader: # iterate over training data in batches\n",
    "            # forward pass\n",
    "            if bert==True:\n",
    "                inputs = tokenizer.batch_encode_plus(inputs, max_length=50, \n",
    "                                                     pad_to_max_length=True, return_tensors='pt', truncation=True)\n",
    "                input_ids = inputs['input_ids'].to(device)\n",
    "                attention_mask = inputs['attention_mask'].to(device)\n",
    "                labels = labels.long()\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                output = bert_classifier(input_ids, attention_mask)\n",
    "            else:\n",
    "                labels = labels.to(dtype=torch.long).to(device) # load batch onto GPU\n",
    "                words = words.to(device) # load batch onto GPU\n",
    "                output = model(words) # make predictions for given inputs\n",
    "\n",
    "            loss = criterion(output, labels) # compare predictions to actual labels\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            loss.backward() # compute gradients using backpropagation\n",
    "            optimizer.step() # update the model weights\n",
    "\n",
    "        if (epoch+1) % 100 == 0: # print loss every 100 epochs\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'final loss: {loss.item():.4f}') # print the final loss after training is complete\n",
    "    criterion = criterion.to(cpu)\n",
    "    model = model.to(cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ac864",
   "metadata": {},
   "source": [
    "First, let's see whether the computer has an GPU available for use with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8f06368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5bfd3f",
   "metadata": {},
   "source": [
    "Now let's instantiate our model, loss function, and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7764f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicNN(input_size, hidden_size, output_size)\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2aad3",
   "metadata": {},
   "source": [
    "Now we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b5f2fba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500], Loss: 0.3349\n",
      "Epoch [200/500], Loss: 0.0000\n",
      "Epoch [300/500], Loss: 0.0000\n",
      "Epoch [400/500], Loss: 0.0107\n",
      "Epoch [500/500], Loss: 0.0001\n",
      "final loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "trainer(model, criterion, optimizer, num_epochs, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a745bf",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "afb6843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = ChatDataset(X_test, y_test)\n",
    "test_loader = DataLoader(dataset=testing_dataset,\n",
    "                          shuffle=False,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3dbcb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(model, test_loader, bert=False):\n",
    "    # evaluation loop\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            if bert==True:\n",
    "                inputs = tokenizer.batch_encode_plus(inputs, max_length=50, \n",
    "                                                     pad_to_max_length=True, return_tensors='pt', truncation=True)\n",
    "                input_ids = inputs['input_ids'].to(device)\n",
    "                attention_mask = inputs['attention_mask'].to(device)\n",
    "                targets = targets.long()\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = bert_classifier(input_ids, attention_mask)\n",
    "            else:\n",
    "                # move the inputs and targets to the device\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                # forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            # update the total number of correct predictions and total number of samples\n",
    "            total_correct += (predictions == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "        # print the accuracy\n",
    "        print(f'Accuracy: {total_correct/total_samples*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f88ae1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.59%\n"
     ]
    }
   ],
   "source": [
    "evaluator(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975cfb20",
   "metadata": {},
   "source": [
    "## Add an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bbe47411",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mapping = {word: i+1 for i, word in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e8c4c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_mapper(sentence, word_to_id, max_length=15):\n",
    "    # map each word to its ID\n",
    "    mapped = [word_to_id.get(word, 1) for word in sentence]\n",
    "\n",
    "    # pad the sequence with zeros up to max_length\n",
    "    if len(mapped) < max_length:\n",
    "        mapped = mapped + [0] * (max_length - len(mapped))\n",
    "    else:\n",
    "        mapped = mapped[:max_length]\n",
    "\n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "42088f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([488,  88, 125,  97,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data2 = []\n",
    "y_data2 = []\n",
    "\n",
    "# for each (X, y) pair in the data\n",
    "for (pattern_sentence, tag) in xy_pairs:\n",
    "    # append sentence where each word is mapped to an id\n",
    "    mapped = word_mapper(pattern_sentence, mapping)\n",
    "    X_data2.append(mapped)\n",
    "    # convert the tag to a label that can be used with PyTorch CrossEntropyLoss\n",
    "    label = tags.index(tag)\n",
    "    # append the label to the y data\n",
    "    y_data2.append(label)\n",
    "\n",
    "# convert X and y to numpy arrays\n",
    "X_data2 = np.array(X_data2)\n",
    "y_data1 = np.array(y_data2)\n",
    "X_data2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "34b7dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into 10% test set and 90% train set\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_data2, y_data2, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ba2a6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset2 = ChatDataset(X_train2, y_train2)\n",
    "train_loader2 = DataLoader(dataset=training_dataset2,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "692003b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, num_classes):\n",
    "        super(EmbeddingNN, self).__init__()\n",
    "        # load pretrained GloVe embeddings\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim=embedding_size)\n",
    "        # the first fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size * embedding_size, hidden_size)\n",
    "        # the second fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # the third fully connected layer\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        # the ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # zero out certain values to help prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        # pass input through the embedding layer\n",
    "        out = self.embedding(x)\n",
    "        print(out.size())\n",
    "        # flatten the output to feed it to the first fully connected layer\n",
    "        out = out.view(out.size(0), -1)\n",
    "        print(out.size())\n",
    "        # apply the ReLU activation function to the first fully connected layer\n",
    "        out = self.relu(self.fc1(out))\n",
    "        # apply dropout regularization to prevent overfitting on the second fully connected layer\n",
    "        out = self.dropout(out)\n",
    "        # apply the ReLU activation function to the second fully connected layer\n",
    "        out = self.relu(self.fc2(out))\n",
    "        # apply dropout regularization to prevent overfitting on the third fully connected layer\n",
    "        out = self.dropout(out)\n",
    "        # apply the third fully connected layer without any activation function\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fb6267c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c58c2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingNN(input_size, embedding_size, hidden_size, output_size)\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "df2ff3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 15, 15])\n",
      "torch.Size([8, 225])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x225 and 7545x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-6350b2185ff9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-79-4f37fa7ce1c9>\u001b[0m in \u001b[0;36mtrainer\u001b[1;34m(model, criterion, optimizer, num_epochs, train_loader, bert)\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# load batch onto GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# load batch onto GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# make predictions for given inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# compare predictions to actual labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-151-8a39948747fc>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# apply the ReLU activation function to the first fully connected layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;31m# apply dropout regularization to prevent overfitting on the second fully connected layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x225 and 7545x32)"
     ]
    }
   ],
   "source": [
    "trainer(model, criterion, optimizer, num_epochs, train_loader2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2f80f",
   "metadata": {},
   "source": [
    "### Not bad! Now let's use a more sophisticated model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6375ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98e70348",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert = []\n",
    "y_bert = []\n",
    "\n",
    "# for each (X, y) pair in the data\n",
    "for (pattern_sentence, tag) in bert_pairs:\n",
    "    X_bert.append(pattern_sentence)\n",
    "    # convert the tag to a label that can be used with PyTorch CrossEntropyLoss\n",
    "    label = tags.index(tag)\n",
    "    # append the label to the y data\n",
    "    y_bert.append(label)\n",
    "\n",
    "# convert X and y to numpy arrays\n",
    "X_bert = np.array(X_bert, dtype=object)\n",
    "y_bert = np.array(y_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4e7f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into 10% test set and 90% train set\n",
    "X_bert_train, X_bert_test, y_bert_train, y_bert_test = train_test_split(X_bert, y_bert, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "187c89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_training_dataset = ChatDataset(X_bert_train, y_bert_train)\n",
    "bert_train_loader = DataLoader(dataset=bert_training_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06e9a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_testing_dataset = ChatDataset(X_bert_test, y_bert_test)\n",
    "bert_test_loader = DataLoader(dataset=bert_testing_dataset,\n",
    "                          shuffle=False,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7af32ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the pre-trained BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bc290d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add a classification layer on top of the pre-trained BERT model\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(768, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        _, pooled_output = out\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(pooled_output)\n",
    "        return self.softmax(linear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "705431ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500], Loss: 2.7496\n",
      "Epoch [200/500], Loss: 2.6246\n",
      "Epoch [300/500], Loss: 2.8746\n",
      "Epoch [400/500], Loss: 2.6246\n",
      "Epoch [500/500], Loss: 2.7496\n"
     ]
    }
   ],
   "source": [
    "bert_classifier = BertClassifier(bert_model, output_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to(device)\n",
    "optimizer = torch.optim.Adam(bert_classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b48c384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = 'bert.pth'\n",
    "torch.save(bert_classifier.state_dict(), bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "867b3859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 14.12%\n"
     ]
    }
   ],
   "source": [
    "evaluator(bert_classifier, bert_test_loader, bert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76216b6f",
   "metadata": {},
   "source": [
    "### Let's Chat!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ca78e",
   "metadata": {},
   "source": [
    "In the below code, the user is prompted for input until they type 'quit' to exit. Assuming the user did not type quit, their input is preprocessed, converted to a bag of words, and fed into the model for prediction. If the model predicts an intent with a high probability, a response is generated based on that intent from a set of predefined responses. If the model predicts with low probability, the chatbot responds with an \"I do not understand...\" message. Thus, the chatbot will (in theory) only respond to questions it can understand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ee57b3ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'quit' to exit)\n",
      "You: who is the professor for this course?\n",
      "Eliot: The professor of CS 472 is Humphrey Shi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-03a8f59fae2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Let's chat! (type 'quit' to exit)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"quit\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             )\n\u001b[1;32m--> 860\u001b[1;33m         return self._input_request(str(prompt),\n\u001b[0m\u001b[0;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Eliot\"\n",
    "print(\"Let's chat! (type 'quit' to exit)\")\n",
    "while True:\n",
    "    sentence = input(\"You: \")\n",
    "    if sentence.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    sentence = preprocess(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    \n",
    "    if prob.item() > 0.9:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "    else:\n",
    "        print(f\"{bot_name}: I'm sorry, but I do not understand. Could you try being more specific?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5d808",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- add spell checking to preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9b25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
