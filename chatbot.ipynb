{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796844c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliot\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spellchecker import SpellChecker\n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e9a11",
   "metadata": {},
   "source": [
    "# Eliot's Interactive Syllabus Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908cf819",
   "metadata": {},
   "source": [
    "## Project Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ee0f2",
   "metadata": {},
   "source": [
    "For my CS 472 final project, I developed an interactive chatbot that serves as an alternative version of the CS 472 syllabus. My chatbot can answer questions related to the class structure, class schedule, class policies, and other syllabus related topics in natural language. The main purpose of my chatbot is to understand and respond to students' natural language queries regarding the CS 472 course material. By offering an interactive way for students to access information about CS 472, I hope to enhance student understanding and engagement with the material, ultimately improving their overall learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d026c3",
   "metadata": {},
   "source": [
    "Code inspired by tutorial created by [Patrick Loeber](https://www.youtube.com/playlist?list=PLqnslRFeH2UrFW4AUgn-eY37qOAWQpJyg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60ea60",
   "metadata": {},
   "source": [
    "<h4 style=\"color:red\">UNCOMMENT THE LINES BELOW IF THIS IS YOUR FIRST TIME USING THIS NOTEBOOK</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a00c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90103d4f",
   "metadata": {},
   "source": [
    "## Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdab2d9",
   "metadata": {},
   "source": [
    "##### How can we make our textual data easier to work with?\n",
    "- Remove \"stop words\" that do not add meaning to the sentence\n",
    "- Map synonyms to a single word to reduce the number of unique features and increase the frequency of important words\n",
    "- Reduce the feature space by stemming each word to its root form\n",
    "- Create a representation of the sentence using techniques such as bag of words or embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827edc7",
   "metadata": {},
   "source": [
    "#### First, let's define our stop words (meaningless words we want to remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103e011",
   "metadata": {},
   "source": [
    "We can modify the list of stop words provided by NLTK to fit our needs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674584d",
   "metadata": {},
   "source": [
    "*Example: what sort of words are in the stop words list?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20a42b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "'do' in stop_words, 'when' in stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84821204",
   "metadata": {},
   "source": [
    "First, what words do we think will help clarify the meaning of sentences that we DO NOT want in our `stop_words` set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c700b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.remove('where')\n",
    "stop_words.remove('when')\n",
    "stop_words.remove('what')\n",
    "stop_words.remove('who')\n",
    "stop_words.remove('how')\n",
    "stop_words.remove('more')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121426e2",
   "metadata": {},
   "source": [
    "Now, what words do we think will not clarify the meaning of sentences that we DO want in our `stop_words` set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106babe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.add('tell')\n",
    "stop_words.add('know')\n",
    "stop_words.add('cs')\n",
    "stop_words.add('cis')\n",
    "stop_words.add('academic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9802e9",
   "metadata": {},
   "source": [
    "#### Spell checking\n",
    "\n",
    "Spelling errors can be a major source of noise in our input data, which can make it more difficult for our model to identify relevant features and patterns. For instance, our model may learn that the term \"professor\" is significant, but it may not recognize that \"profesor\" has the same meaning.\n",
    "\n",
    "To improve the quality of our input data, we can incorporate logic to spell check words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d5ffe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_check = SpellChecker(language='en', case_sensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4583ae8",
   "metadata": {},
   "source": [
    "*Example usage:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0a580e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thiss', 'is', 'a', 'sample', 'text', 'with', 'some', 'spelling', 'errors']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a sample text with some spelling errors\n",
    "text = \"Thiss is a sampel text with some speling erors.\"\n",
    "\n",
    "# split the text into individual words\n",
    "words = text.split()\n",
    "\n",
    "words = [spell_check.correction(word) for word in words]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11048b",
   "metadata": {},
   "source": [
    "#### Synonym Replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a904335b",
   "metadata": {},
   "source": [
    "Let's make a list of useful synonyms to help us replace them with a single word. This will help us reduce the number of features and highlight important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1ba9a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {\n",
    "    'location': {'room', 'classroom', 'building', 'place', 'site', 'hall'},\n",
    "    'disability': {'injury', 'disorder', 'condition', 'impairment', 'impaired',\n",
    "                  'injured', 'ptsd', 'anxiety', 'dyslexia', 'adhd', 'depression',\n",
    "                  'illness', 'disease', 'autism'},\n",
    "    'accomadate': {'support', 'supported', 'supporting', 'supports',\n",
    "             'accomadate', 'accomadates', 'accommodation', 'accomadated' \n",
    "             'assistance', 'assist', 'assisting', 'assists'},\n",
    "    'more': {'added', 'additional', 'extra'},\n",
    "    'zoom': {'remote'},\n",
    "    'ml': {'ai', 'rnn', 'cnn', 'PCA', 'dimensionality', 'regression', 'knn', \n",
    "           'svm', 'svms', 'gans', 'gan', 'optimization', 'cnns', 'transformers',\n",
    "          'vaes', 'vae', 'regularization', 'gradient', 'neural', 'network'},\n",
    "    'test': {'exam', 'test'},\n",
    "    'dishonesty': {'dishonesty', 'cheating', 'plagiarize', 'copying', 'copied',\n",
    "                   'collusion', 'plagiarism', 'lying', 'cheat'},\n",
    "    '315': {'315', 'cs315', 'cis315', 'cs415', '415', '313', 'cs313', 'cis313', \n",
    "           'cis415', 'cs314', '314', 'cis314', 'cs212', 'cis212', '212',\n",
    "           'cs211', 'cis211', '211', 'cs210', 'cis210', '210',\n",
    "           'cs425', 'cis425', '425', 'cs471', 'cis471', '471', \n",
    "           'cs330', 'cis330', '330'},\n",
    "    'course': {'cs472', 'class', 'course', '472', 'cis472', 'curriculum', 'lecture',\n",
    "              'lectures'},\n",
    "    'content': {'subject', 'topic', 'concepts'},\n",
    "    'cs': {'cis', 'cs'},\n",
    "    'you': {'ya', 'you'},\n",
    "    'time': {'monday', 'tuesday', 'wednesday', \n",
    "             'thursday', 'friday','saturday', 'sunday',\n",
    "             'tomorrow', 'yesterday', 'today', 'january', 'february', \n",
    "             'march', 'april', 'may', 'june', 'july', 'august',\n",
    "             'september', 'october', 'november', 'december',\n",
    "             'tonight', 'afternoon', 'tonight', 'morning'},\n",
    "    'software': {'pytorch', 'tensorflow', 'numpy', 'pandas', \n",
    "               'language', 'framework', 'javascript', 'c',\n",
    "              'sklearn', 'scikitlearn', 'keras', 'jupyter', 'python', 'r'},\n",
    "    'sick': {'ill', 'fever', 'illness', 'flu', 'covid', 'covid19', 'nausea', 'cough'\n",
    "            'headache', 'diarrhea', 'congestion', 'sickness'},\n",
    "    'coding': {'programming', 'code', 'program', 'develop', 'developing'},\n",
    "    'group': {'team'},\n",
    "    'start': ['begin', 'commence'],\n",
    "    'computer': {'pc', 'mac', 'laptop'},\n",
    "    'employee': {'employee', 'worker', 'staff'},\n",
    "    'ta': {'ge', 'steven', 'walton'},\n",
    "    'good': {'great', 'nice', 'awesome', 'cool'},\n",
    "    'goodbye': {'adieu', 'farewell', \n",
    "                'bye', 'adios', 'arrivederci', \n",
    "                'auf', 'ciao', 'later', 'peace', \n",
    "                'sayonara', 'see ya', 'ttyl', \n",
    "                'wiedersehen'},\n",
    "    'grad': {'graduate'},\n",
    "    'hi': {'hello', 'hey', 'hiya', 'hola', 'greetings', 'yo'},\n",
    "    'i': {'myself'},\n",
    "    'long': {'lengthy'},\n",
    "    'prerequisites': {'prereqs', 'prerequisite'},\n",
    "    'professor': {'teacher', 'instructor', 'prof', 'humphrey', 'shi'},\n",
    "    'homework': {'hw', 'assignment'},\n",
    "    'grade': {'evaluate', 'assess', 'score'},\n",
    "    'require': {'need', 'necessary', \"mandatory\", \"compulsory\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ff57b",
   "metadata": {},
   "source": [
    "Now that we have a dictionary that we can use to map synonyms to a single word, we can create a function to perform our synonym replacement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "da8f2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonymReplacer(tokens, stemmed=False):\n",
    "    # loop through each token in the list\n",
    "    for i, word in enumerate(tokens):\n",
    "        # convert the word to lowercase\n",
    "        word = word.lower()\n",
    "        # loop through each synonym list in the synonyms dictionary\n",
    "        for syn, syn_list in synonyms.items():\n",
    "            # check if the current word is in the synonym list\n",
    "            if stemmed:\n",
    "                syn_list = [stemmer.stem(w) for w in syn_list]\n",
    "            if word in syn_list:\n",
    "                # replace the current token with the synonym\n",
    "                tokens[i] = syn\n",
    "                # break out of the loop since we've found a synonym\n",
    "                break\n",
    "    # return the modified list of tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f462e",
   "metadata": {},
   "source": [
    "*Example synonym replacement:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7bb7f140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when', 'does', 'course', 'start']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonymReplacer([\"when\", \"does\", \"class\", \"begin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fef585",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Stemming reduces words to their root form by removing parts of the word like prefixes and suffixes. This also helps to reduce the feature space as well as increase the frequency of similar words (like \"run\", \"running\", and \"runs\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "008f4b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the porter stemmer from NLTK\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02883f95",
   "metadata": {},
   "source": [
    "*Example of stemming*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aa42bd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'run', 'run']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(word) for word in [\"run\", \"running\", \"runs\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd099d",
   "metadata": {},
   "source": [
    "#### A Preprocessing Function\n",
    "\n",
    "Now we can combine the preprocessing techniques described above into a single function that we can use to remove noise and irrelevant information from our data.\n",
    "\n",
    "The preprocess function in the cell below takes a sentence as input, removes punctuation and stop words, stems each word in the sentence, maps synonymous words to a single word, and returns the preprocessed sentence as a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c9b38b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence, remove_stop=True):\n",
    "    \"\"\"\n",
    "    This function takes a sentence as input and performs various text preprocessing steps on it,\n",
    "    including removing punctuation, stop words, and stemming each word in the sentence.\n",
    "    \"\"\"\n",
    "    # remove punctuation from sentence\n",
    "    sentence = ''.join(\n",
    "        char for char in sentence if char not in string.punctuation\n",
    "    )\n",
    "    # tokenizing the sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # try to correct spelling issues\n",
    "    tokens_checked = []\n",
    "    for word in tokens:\n",
    "        corrected_word = spell_check.correction(word)\n",
    "        if corrected_word is not None:\n",
    "            tokens_checked.append(corrected_word)\n",
    "        else:\n",
    "            tokens_checked.append(word)\n",
    "    # replace synonyms\n",
    "    tokens_checked = synonymReplacer(tokens_checked)\n",
    "    if remove_stop:\n",
    "        # removing stop words\n",
    "        tokens_checked = [\n",
    "            token for token in tokens_checked if token.lower() not in stop_words\n",
    "        ]\n",
    "    # stemming each word in the sentence\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens_checked]\n",
    "    stemmed_words = synonymReplacer(stemmed_words, stemmed=True)\n",
    "    # return the preprocessed sentence as a list of words\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1242c",
   "metadata": {},
   "source": [
    "*Preprocessing Example*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c766d",
   "metadata": {},
   "source": [
    "What exactly does our preprocessing do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a497c33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who', 'lead', 'course', 'course']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"Can you tell me who will be leeding the letures for CS 472?\"\n",
    "tokens = preprocess(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f296c5d",
   "metadata": {},
   "source": [
    "First, the `preprocess` function removes all punctuation marks from the sentence using the `string.punctuation` module. Then, the sentence is tokenized into a list of words using the nltk.word_tokenize method.\n",
    "\n",
    "Next, the function removes stop words, which are common words that do not carry much meaning in the sentence, such as \"a\", \"an\", \"the\", \"of\", and so on. In this case, the function is using a pre-defined list of stop words to remove them from the list of tokens.\n",
    "\n",
    "After that, the function performs stemming on each word in the sentence, which involves converting the words into their root or base form, called their stem. The function uses a stemmer to perform this task.\n",
    "\n",
    "Finally, the preprocessed words are returned as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590864b",
   "metadata": {},
   "source": [
    "#### A bag-of-words representation of a sentence\n",
    "\n",
    "The main goal of the `bag_of_words` function is to convert a sentence into a numerical representation that captures the presence or absence of each known word in the vocabulary of known words (we will build our vocabulary soon!).\n",
    "\n",
    "Here is how the function works:\n",
    "\n",
    "The bag_of_words function takes a tokenized sentence and a list of all known words in the vocabulary as input, and creates a bag of words representation for the given sentence. It initializes the bag with zeros for each word in the vocabulary, and updates the bag with 1 for each word in the sentence that exists in the vocabulary. The function returns a numpy array representing the bag of words with 1 for each known word that exists in the sentence, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "931ede3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    Create a bag of words representation for a given tokenized sentence.\n",
    "    \"\"\"\n",
    "    # initialize the bag with zeros for each word in the vocabulary\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "\n",
    "    # update the bag with 1 for each word in the sentence that exists in the vocabulary\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in tokenized_sentence: \n",
    "            bag[idx] = 1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cc1617bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who', 'lead', 'course', 'course']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "67a32bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_all_words = ['hello', 'tell', 'what', 'ten', 'lead', 'lectur', 'cs', 'who', '472']\n",
    "bag_of_words(tokens, example_all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe04518",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6081592",
   "metadata": {},
   "source": [
    "We will now load our data from the intents file and preprocess its content using the logic we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "37011752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data file as a Python object\n",
    "with open('intents.json', 'r') as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# empty lists and dictionary for storing the tokenized words, tags, and (X, y) pairs\n",
    "all_words = []  # no stop words\n",
    "all_words_stop = []  # stop words included\n",
    "tags = []\n",
    "xy_pairs = []  # no stop words\n",
    "xy_pairs_stop = []  # stop words included\n",
    "\n",
    "# loop through each intent in the data file\n",
    "for intent in intents['intents']:\n",
    "    # get the tag\n",
    "    tag = intent['tag']\n",
    "    # add the tag to our list of tags\n",
    "    tags.append(tag)\n",
    "    # loop through each pattern (sentence) in the intent\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize the pattern into a list of words\n",
    "        words = preprocess(pattern)\n",
    "        words_stop = preprocess(pattern, remove_stop=False)\n",
    "        # loop through each word in the tokenized pattern\n",
    "        for word in words:\n",
    "            # add the word to our list of all words\n",
    "            all_words.append(word)\n",
    "        for word in words_stop:\n",
    "            # add the word to our list of all words\n",
    "            all_words_stop.append(word)\n",
    "        # add the (words, tag) pair to our list of (X, y) pairs\n",
    "        xy_pairs.append((words, tag))\n",
    "        xy_pairs_stop.append((words_stop, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa8032",
   "metadata": {},
   "source": [
    "#### Vocabulary Sets\n",
    "\n",
    "Now that we have a list of all words, we have a vocabulary for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "56014206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(649, 733)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicate words and sort the list\n",
    "all_words = sorted(set(all_words))\n",
    "all_words_stop = sorted(set(all_words_stop))\n",
    "# sort the list of tags\n",
    "tags = sorted(set(tags))\n",
    "len(all_words), len(all_words_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "da272362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GE',\n",
       " 'accomadations',\n",
       " 'attendance',\n",
       " 'cheatingConsequences',\n",
       " 'cheatingPolicy',\n",
       " 'cheatingReporting',\n",
       " 'covid',\n",
       " 'finalDetails',\n",
       " 'goodbye',\n",
       " 'grading',\n",
       " 'greeting',\n",
       " 'lab',\n",
       " 'language',\n",
       " 'lateWork',\n",
       " 'lectureLocation',\n",
       " 'lectures',\n",
       " 'midtermContent',\n",
       " 'midtermDetails',\n",
       " 'officeHours',\n",
       " 'prerequisites',\n",
       " 'professor',\n",
       " 'project',\n",
       " 'projectGroups',\n",
       " 'textbook',\n",
       " 'thanks',\n",
       " 'topics',\n",
       " 'zoom']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219782c5",
   "metadata": {},
   "source": [
    "#### Create the Training and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661f3b4",
   "metadata": {},
   "source": [
    "The code in the cell below creates the training data by converting each input sentence in the original data to a bag of words representation using the bag_of_words function, and then converting the intent tags to integer labels that can be used for training. The resulting training data is represented as two numpy arrays, X_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5333d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data1 = []\n",
    "y_data1 = []\n",
    "\n",
    "# for each (X, y) pair in the data\n",
    "for (pattern_sentence, tag) in xy_pairs:\n",
    "    # create a bag of words for the pattern sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    # append the bag of words to the X data\n",
    "    X_data1.append(bag)\n",
    "    # convert the tag to a label that can be used with PyTorch CrossEntropyLoss\n",
    "    label = tags.index(tag)\n",
    "    # append the label to the y data\n",
    "    y_data1.append(label)\n",
    "\n",
    "# convert X and y to numpy arrays\n",
    "X_data1 = np.array(X_data1)\n",
    "y_data1 = np.array(y_data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3ccf1",
   "metadata": {},
   "source": [
    "Now we can get training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f2a6b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into 10% test set and 90% train set\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_data1, y_data1, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18e965",
   "metadata": {},
   "source": [
    "## Create The Model \n",
    "\n",
    "**NOTE**: The hyperparameters below were not chosen at random, but through manual testing. For instance, I tried the following hidden layer sizes: 4, 8, 16, 32, 64, 128, 256. I found that 32 produced the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf5812",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4eb6010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649 27\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size1 = len(X_train1[0])\n",
    "hidden_size = 32\n",
    "output_size = len(tags)\n",
    "max_seq_length = 50\n",
    "print(input_size1, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e022bc",
   "metadata": {},
   "source": [
    "#### Model architecture\n",
    "In order to effectively capture the underlying patterns in language data, a model capable of learning complex relationships is required. However, it is important to strike a balance and avoid overfitting by not making the model more complex than necessary. To start, I chose to create a simple feed forward neural network.\n",
    "\n",
    "My first model in the cell below uses three fully connected layers, providing the capability to learn complex patterns in the input data. The ReLU activation function between the first and second fully connected layers introduces non-linearity into the model, improving its ability to model complex relationships. The third fully connected layer maps the learned features to the output classes, and no activation function is used for this layer. \n",
    "\n",
    "#### Addressing overfitting - dropout layers\n",
    "Dropout is a regularization technique that randomly drops a certain percentage of the neurons in a neural network during training. By dropping out some neurons, the remaining neurons must learn to compensate for the missing ones, which encourages them to be more independent.\n",
    "\n",
    "Essentially, adding dropout layers to my model will help it to learn more generalized features that and helps prevent the model from learning noise in the training data.\n",
    "\n",
    "[source](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ab67f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(BasicNN, self).__init__()\n",
    "        # the first fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # the second fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) \n",
    "        # the third fully connected layer\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        # the ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # zero out certain values to help prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # apply the ReLU activation function to the first fully connected layer\n",
    "        out = self.relu(self.fc1(x))  # apply relu to the 1st fully connected layer output\n",
    "        # apply dropout regularization to prevent overfitting on the 2nd fully connected layer\n",
    "        out = self.dropout(out)\n",
    "        # apply the ReLU activation function to the second fully connected layer\n",
    "        out = self.relu(self.fc2(out))  # apply relu to the 2nd fully connected layer output\n",
    "        # apply dropout regularization to prevent overfitting on the 3rd fully connected layer\n",
    "        out = self.dropout(out)\n",
    "        # apply the third fully connected layer without any activation function\n",
    "        out = self.fc3(out)  # apply the last fully connected layer\n",
    "        return out  # return the final output of the network\n",
    "    \n",
    "    def sentence_representation(X):\n",
    "        # FIXME\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c668e5",
   "metadata": {},
   "source": [
    "#### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821aed6",
   "metadata": {},
   "source": [
    "The `ChatDataset` class is a PyTorch dataset object that is designed to be used with PyTorch's DataLoader module to retrieve training data and corresponding labels for my chatbot application. The purpose of this class is to encapsulate the training data and labels as attributes of the object and to define the methods `__getitem__()` and `__len__()` to support indexing and length operations, respectively, on the dataset object. This class is an implementation of the Dataset abstract class in PyTorch and provides a consistent interface for loading training data for use with PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3415550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset object that is designed to be used with DataLoader to retrieve\n",
    "    training data and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Initializes the ChatDataset object.\n",
    "        \"\"\"\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns the specified training sample.\n",
    "        \"\"\"\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of training samples.\n",
    "        \"\"\"\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7268998",
   "metadata": {},
   "source": [
    "This code creates a `ChatDataset` object with training data and corresponding labels `X_train` and `y_train`, and then uses a PyTorch DataLoader object to iterate over the dataset during model training. The `DataLoader` is configured to retrieve data in batches of size `batch_size`, to shuffle the data before each epoch of training, and to use 0 worker processes for data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9ec14acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset1 = ChatDataset(X_train1, y_train1)\n",
    "train_loader1 = DataLoader(dataset=training_dataset1,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d91d43",
   "metadata": {},
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f657555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, num_epochs, train_loader, bert=False):\n",
    "    # move loss function and model to gpu\n",
    "    criterion = criterion.to(device)\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs): # loop over the specified number of epochs\n",
    "        for (words, labels) in train_loader: # iterate over training data in batches\n",
    "            # forward pass\n",
    "            if bert==True:\n",
    "                inputs = tokenizer.batch_encode_plus(inputs, max_length=50, \n",
    "                                                     pad_to_max_length=True, return_tensors='pt', truncation=True)\n",
    "                input_ids = inputs['input_ids'].to(device)\n",
    "                attention_mask = inputs['attention_mask'].to(device)\n",
    "                labels = labels.long()\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                output = bert_classifier(input_ids, attention_mask)\n",
    "            else:\n",
    "                labels = labels.to(dtype=torch.long).to(device) # load batch onto gpu\n",
    "                words = words.to(device) # load batch onto GPU\n",
    "                output = model(words) # make predictions for given inputs\n",
    "\n",
    "            loss = criterion(output, labels) # compare predictions to actual labels\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            loss.backward() # compute gradients using backpropagation\n",
    "            optimizer.step() # update the model weights\n",
    "\n",
    "        if (epoch+1) % 100 == 0: # print loss every 100 epochs\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'final loss: {loss.item():.4f}') # print the final loss after training is complete\n",
    "    \n",
    "    # move back to cpu to free up gpu memory\n",
    "    criterion = criterion.to(cpu)\n",
    "    model = model.to(cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ac864",
   "metadata": {},
   "source": [
    "First, let's see whether the computer has an GPU available for use with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8f06368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5bfd3f",
   "metadata": {},
   "source": [
    "Now let's instantiate our model, loss function, and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7764f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = BasicNN(input_size1, hidden_size, output_size)\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2aad3",
   "metadata": {},
   "source": [
    "Now we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b5f2fba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500], Loss: 0.0121\n",
      "Epoch [200/500], Loss: 0.0033\n",
      "Epoch [300/500], Loss: 0.0083\n",
      "Epoch [400/500], Loss: 0.0004\n",
      "Epoch [500/500], Loss: 0.0490\n",
      "final loss: 0.0490\n"
     ]
    }
   ],
   "source": [
    "trainer(model1, criterion, optimizer, num_epochs, train_loader1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a745bf",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "afb6843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset1 = ChatDataset(X_test1, y_test1)\n",
    "test_loader1 = DataLoader(dataset=testing_dataset1,\n",
    "                          shuffle=False,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3dbcb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(model, test_loader, bert=False):\n",
    "    # evaluation loop\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            if bert==True:\n",
    "                inputs = tokenizer.batch_encode_plus(inputs, max_length=50, \n",
    "                                                     pad_to_max_length=True, return_tensors='pt', truncation=True)\n",
    "                input_ids = inputs['input_ids'].to(device)\n",
    "                attention_mask = inputs['attention_mask'].to(device)\n",
    "                targets = targets.long()\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = bert_classifier(input_ids, attention_mask)\n",
    "            else:\n",
    "                # move the inputs and targets to the device\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                # forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            # update the total number of correct predictions and total number of samples\n",
    "            total_correct += (predictions == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "        # print the accuracy\n",
    "        print(f'Accuracy: {total_correct/total_samples*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f88ae1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.60%\n"
     ]
    }
   ],
   "source": [
    "evaluator(model1, test_loader1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975cfb20",
   "metadata": {},
   "source": [
    "## Add an Embedding Layer to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e618a83",
   "metadata": {},
   "source": [
    "#### What are the down sides of a bag-of-words representation of a sentence?\n",
    "\n",
    "The primary downside of the bag-of-words representation is that it assumes that each word occurs independently of all other words, which of course is unrealistic. Thus, the following issues arise:\n",
    "- The bag-of-words representation cannot capture relationships between words\n",
    "- The bag-of-words representation cannot handle words outside of its vocabulary\n",
    "\n",
    "[Source](https://medium.com/swlh/word-embeddings-versus-bag-of-words-the-curious-case-of-recommender-systems-6ac1604d4424)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845980f9",
   "metadata": {},
   "source": [
    "#### How does an embedding representation help to address these issues?\n",
    "\n",
    "The embedding representation is capable of considering the context of a sentence a word is in. This can lead to improved performance as...\n",
    "- Embedding representations allow for better generalization as they do not only rely on words appearing that they already know, but can recognize similar semantic patterns, too\n",
    "- This means it can map words outside of its vocabulary by mapping these words to other words in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b0ab1",
   "metadata": {},
   "source": [
    "### Include Stop Words?\n",
    "\n",
    "I am curious as to whether my network with an embedding layer will perform better with stop words included or with stop words excluded and compare the accuracies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951e267",
   "metadata": {},
   "source": [
    "#### A slightly modified preprocessing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8863323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_with_stopwords = 35\n",
    "max_length_no_stopwords = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f91ef3",
   "metadata": {},
   "source": [
    "We first want to create a mapping between words in our vocabulary and their corresponding integer indices. \n",
    "\n",
    "NOTE: I also add a special null word to the mapping dictionary, which is represented by the string \"123NULLWORD123\". This token is used to pad sequences to a fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bbe47411",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with stop words\n",
    "mapping1 = {word: i for i, word in enumerate(all_words_stop)}\n",
    "mapping1[\"123NULLWORD123\"] = len(all_words_stop)\n",
    "input_size2 = len(mapping1)\n",
    "\n",
    "# without stopwords\n",
    "mapping2 = {word: i for i, word in enumerate(all_words)}\n",
    "mapping2[\"123NULLWORD123\"] = len(all_words)\n",
    "input_size3 = len(mapping2)\n",
    "input_size3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da214060",
   "metadata": {},
   "source": [
    "Next, let's make a new function `word_mapper`, to assist with our preprocessing. `word_mapper` takes a sentence, the `mapping` dictionary, and a maximum sequence length as inputs. First, `word_mapper` maps each word in the sentence to its corresponding index in the `mapping` dictionary. Then, `word_mapper` pads the resulting list with the index of the last word if its length is less than `max_length`. Finally, the resulting list of indices is returned, representing the input sentence encoded as a sequence of integers with padding. We can use this encoded sequence as input to an embedding layer in our new neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e8c4c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_mapper(sentence, word_to_id, input_size, max_length=15):\n",
    "    # map each word to its id\n",
    "    mapped = [word_to_id.get(word, 1) for word in sentence]\n",
    "\n",
    "    # pad the sequence with number out of sequence up to max_length\n",
    "    if len(mapped) < max_length:\n",
    "        mapped = mapped + [input_size-1] * (max_length - len(mapped))\n",
    "    else:\n",
    "        mapped = mapped[:max_length]\n",
    "\n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22161987",
   "metadata": {},
   "source": [
    "#### Picking an embedding size\n",
    "\n",
    "It is tempting to pick an embedding size equal to the max length of the sentence. Afterall, could we not capture the underlying context of a word in a sentence using a vector of the length of the sentence and mark where it falls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "53aa0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36f1bf",
   "metadata": {},
   "source": [
    "#### Build our dataset like usual..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "42088f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with stop words\n",
    "X_data2 = []\n",
    "y_data2 = []\n",
    "\n",
    "# for each (X, y) pair in the data\n",
    "for (pattern_sentence, tag) in xy_pairs_stop:\n",
    "    # append sentence where each word is mapped to an id\n",
    "    mapped = word_mapper(pattern_sentence, mapping1, input_size2, max_length=max_length_with_stopwords)\n",
    "    X_data2.append(mapped)\n",
    "    # convert the tag to a label that can be used with PyTorch CrossEntropyLoss\n",
    "    label = tags.index(tag)\n",
    "    # append the label to the y data\n",
    "    y_data2.append(label)\n",
    "\n",
    "# convert X and y to numpy arrays\n",
    "X_data2 = np.array(X_data2)\n",
    "y_data2 = np.array(y_data2)\n",
    "\n",
    "# ==============================================================================\n",
    "# with NO stop words\n",
    "X_data3 = []\n",
    "y_data3 = []\n",
    "\n",
    "# for each (X, y) pair in the data\n",
    "for (pattern_sentence, tag) in xy_pairs:\n",
    "    # append sentence where each word is mapped to an id\n",
    "    mapped = word_mapper(pattern_sentence, mapping2, input_size3, max_length=max_length_no_stopwords)\n",
    "    X_data3.append(mapped)\n",
    "    # convert the tag to a label that can be used with PyTorch CrossEntropyLoss\n",
    "    label = tags.index(tag)\n",
    "    # append the label to the y data\n",
    "    y_data3.append(label)\n",
    "\n",
    "# convert X and y to numpy arrays\n",
    "X_data3 = np.array(X_data3)\n",
    "y_data3 = np.array(y_data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bbd83",
   "metadata": {},
   "source": [
    "#### Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "34b7dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data with stop words into 10% test set and 90% train set\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_data2, y_data2, test_size=0.1)\n",
    "\n",
    "# split the data without stop words into 10% test set and 90% train set\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_data3, y_data3, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ba2a6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words included\n",
    "training_dataset2 = ChatDataset(X_train2, y_train2)\n",
    "train_loader2 = DataLoader(dataset=training_dataset2,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)\n",
    "\n",
    "# stop words excluded\n",
    "training_dataset3 = ChatDataset(X_train3, y_train3)\n",
    "train_loader3 = DataLoader(dataset=training_dataset3,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "692003b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, max_length):\n",
    "        super(EmbeddingNN, self).__init__()\n",
    "        # load pretrained GloVe embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_length = max_length\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                      embedding_dim=embedding_size)\n",
    "        # the first fully connected layer\n",
    "        self.fc1 = nn.Linear(max_length*embedding_dim, 256)\n",
    "        # the second fully connected layer\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        # the ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # zero out certain values to help prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.85)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        # pass input through the embedding layer\n",
    "        out = self.embedding(x)\n",
    "        # flatten the output to feed it to the first fully connected layer\n",
    "        out = out.view(-1, self.max_length, self.embedding_dim)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # apply the ReLU activation function to the first fully connected layer\n",
    "        out = self.relu(self.fc1(out))\n",
    "        # apply dropout regularization to prevent overfitting on the second fully connected layer\n",
    "        out = self.dropout(out)\n",
    "        # apply the ReLU activation function to the second fully connected layer\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff4921",
   "metadata": {},
   "source": [
    "#### Train and evaluate model with stop words *INCLUDED* in input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c58c2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = EmbeddingNN(input_size2, embedding_size, output_size, max_length_with_stopwords)\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "df2ff3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500], Loss: 1.1301\n",
      "Epoch [200/500], Loss: 1.1499\n",
      "Epoch [300/500], Loss: 0.4882\n",
      "Epoch [400/500], Loss: 0.1465\n",
      "Epoch [500/500], Loss: 0.0002\n",
      "final loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "trainer(model2, criterion, optimizer, num_epochs, train_loader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8b8e14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset2 = ChatDataset(X_test2, y_test2)\n",
    "test_loader2 = DataLoader(dataset=testing_dataset2,\n",
    "                          shuffle=False,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2e6474a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.67%\n"
     ]
    }
   ],
   "source": [
    "evaluator(model2, test_loader2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49192fc6",
   "metadata": {},
   "source": [
    "#### Train and evaluate model with stop words *EXCLUDED* from input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a81fd7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = EmbeddingNN(input_size3, embedding_size, output_size, max_length_no_stopwords)\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c8d1e07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0209\n",
      "Epoch [200/1000], Loss: 0.0029\n",
      "Epoch [300/1000], Loss: 0.1280\n",
      "Epoch [400/1000], Loss: 0.4169\n",
      "Epoch [500/1000], Loss: 0.0000\n",
      "Epoch [600/1000], Loss: 0.0006\n",
      "Epoch [700/1000], Loss: 0.0170\n",
      "Epoch [800/1000], Loss: 0.0009\n",
      "Epoch [900/1000], Loss: 0.0000\n",
      "Epoch [1000/1000], Loss: 0.1469\n",
      "final loss: 0.1469\n"
     ]
    }
   ],
   "source": [
    "trainer(model3, criterion, optimizer, num_epochs, train_loader3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d0348d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset3 = ChatDataset(X_test3, y_test3)\n",
    "test_loader3 = DataLoader(dataset=testing_dataset3,\n",
    "                          shuffle=False,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b8a8b846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.03%\n"
     ]
    }
   ],
   "source": [
    "evaluator(model3, test_loader3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e0ae2",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "- Including stop words hurts model accuracy significantly.\n",
    "- The neural network with an embedding layer was highly accurate, but not more accurate than the simple feed forward neural net. My guess for why this is that the sentences are simple enough and similar enough that an embedding layer does not add much value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238caf11",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55a7b1",
   "metadata": {},
   "source": [
    "FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76216b6f",
   "metadata": {},
   "source": [
    "### Let's Chat!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ca78e",
   "metadata": {},
   "source": [
    "In the below code, the user is prompted for input until they type 'quit' to exit. Assuming the user did not type quit, their input is preprocessed, converted to a bag of words, and fed into the model for prediction. If the model predicts an intent with a high probability, a response is generated based on that intent from a set of predefined responses. If the model predicts with low probability, the chatbot responds with an \"I do not understand...\" message. Thus, the chatbot will (in theory) only respond to questions it can understand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ee57b3ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set the model to evaluation mode\n",
    "def chat(model, embedding=False):\n",
    "    model.to(cpu)\n",
    "    model.eval()\n",
    "\n",
    "    bot_name = \"Eliot\"\n",
    "    print(\"Let's chat! (type 'quit' to exit)\")\n",
    "    while True:\n",
    "        sentence = input(\"You: \")\n",
    "        if sentence.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        sentence = preprocess(sentence)\n",
    "        if embedding:\n",
    "            X = word_mapper(sentence, \n",
    "                            mapping2, \n",
    "                            input_size3, \n",
    "                            max_length=max_length_no_stopwords\n",
    "                           )\n",
    "            X = np.array(X)\n",
    "            X = torch.from_numpy(X)\n",
    "        else:\n",
    "            X = bag_of_words(sentence, all_words)\n",
    "            X = X.reshape(1, X.shape[0])\n",
    "            X = torch.from_numpy(X)\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "\n",
    "        if prob.item() > 0.9:\n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: I'm sorry, but I do not understand. Could you try being more specific?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "eae14d69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'quit' to exit)\n",
      "You: hi\n",
      "Eliot: Hi there, what can I do for you?\n",
      "You: can i submit work late?\n",
      "Eliot: I do not know if there is a penalty for submitting work late. I'm Sorry.\n",
      "You: what should i do if i suspect my group member of cheating?\n",
      "Eliot: Cases of dishonesty will be handled according to university policy. Students are responsible for taking reasonable precautions to protect their work, and turning in someone else's code is considered collusion and will result in a failing grade for all involved parties.\n",
      "You: when is class?\n",
      "Eliot: Two lectures are delivered in person every week on Tuesdays and Thursdays, from 4:00 PM to 5:20 PM\n",
      "You: when is the final exam?\n",
      "Eliot: CS 472 does not have a final exam\n",
      "You: do i have to go to class?\n",
      "Eliot: It is highly recommended that you attend lecture. However, it is not required.\n",
      "You: i feel ill. what should i do?\n",
      "Eliot: Hi there, how can I help?\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "chat(model1, embedding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5d808",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9b25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
