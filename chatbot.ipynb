{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "796844c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eliot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eliot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\eliot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eliot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e9a11",
   "metadata": {},
   "source": [
    "# Eliot's Interactive Syllabus Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d026c3",
   "metadata": {},
   "source": [
    "Code inspired by tutorial created by [Patrick Loeber](https://www.youtube.com/playlist?list=PLqnslRFeH2UrFW4AUgn-eY37qOAWQpJyg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90103d4f",
   "metadata": {},
   "source": [
    "### Preparing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ab370",
   "metadata": {},
   "source": [
    "The code in the cell below defines two functions to preprocess text data and create a bag of words representation for a given sentence.\n",
    "\n",
    "The preprocess function takes a sentence as input, removes punctuation and stop words, stems each word in the sentence, and returns the preprocessed sentence as a list of words.\n",
    "\n",
    "The bag_of_words function takes a tokenized sentence and a list of all known words in the vocabulary as input, and creates a bag of words representation for the given sentence. It initializes the bag with zeros for each word in the vocabulary, and updates the bag with 1 for each word in the sentence that exists in the vocabulary. The function returns a numpy array representing the bag of words with 1 for each known word that exists in the sentence, 0 otherwise.\n",
    "\n",
    "The techniques in used in these functions are commonly used in natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369c0bf",
   "metadata": {},
   "source": [
    "NOTE: the stop words list provided by NLTK includes words that I don't think are stop words, and it also doesn't include some words that I do think are stop words for the purposes of this project. Thus, I start by modifying the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57c700b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('where')\n",
    "stop_words.remove('when')\n",
    "stop_words.remove('what')\n",
    "stop_words.remove('who')\n",
    "stop_words.add('tell')\n",
    "stop_words.add('know')\n",
    "stop_words.add('cs')\n",
    "stop_words.add('cis')\n",
    "stop_words.add('academic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015f91b",
   "metadata": {},
   "source": [
    "Next, let's create a dictionary of meaningful synonyms. By replacing all synonyms with a single word, we can reduce the number of features in our feature space while increasing the frequency of important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ba9a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {\n",
    "    'disability': {'injury', 'disorder', 'condition', 'impairment', 'impaired',\n",
    "                  'injured'},\n",
    "    'zoom': {'remote'},\n",
    "    'ml': {'ai', 'rnn', 'cnn', 'PCA', 'dimensionality', 'regression', 'knn', \n",
    "           'svm', 'svms', 'gans', 'gan', 'optimization', 'cnns', 'transformers',\n",
    "          'vaes', 'vae', 'regularization', 'gradient', 'neural', 'network'},\n",
    "    'test': {'exam', 'test', 'midterm'},\n",
    "    'dishonesty': {'dishonesty', 'cheating', 'plagiarize', 'copying', 'copied',\n",
    "                   'collusion', 'plagiarism', 'lying', 'cheat'},\n",
    "    '315': {'315', 'cs315', 'cis315'},\n",
    "    'course': ['cs472', 'class', 'course', '472'],\n",
    "    'cs': {'cis', 'cs'},\n",
    "    'you': {'ya', 'you'},\n",
    "    'weekday': {'monday', 'tuesday', 'wednesday', \n",
    "                'thursday', 'friday','saturday', 'sunday'},\n",
    "    'month': {'january', 'february', 'march', 'april', \n",
    "              'may', 'june', 'july', 'august',\n",
    "             'september', 'october', 'november', 'december'},\n",
    "    'software': {'pytorch', 'tensorflow', 'numpy', 'pandas', \n",
    "               'language', 'framework', 'javascript', 'c',\n",
    "              'sklearn', 'scikitlearn', 'keras', 'jupyter', 'python'},\n",
    "    'coding': {'programming', 'code'},\n",
    "    'start': ['start', 'begin', 'commence'],\n",
    "    'computer': {'computer', 'pc', 'mac', 'laptop'},\n",
    "    'employee': {'employee', 'worker', 'staff'},\n",
    "    'ta': {'ge', 'ta'},\n",
    "    'good': {'good', 'great', 'nice', 'awesome', 'cool'},\n",
    "    'goodbye': {'adieu', 'goodbye', 'farewell', \n",
    "                'bye', 'adios', 'arrivederci', \n",
    "                'auf', 'ciao', 'later', 'peace', \n",
    "                'sayonara', 'see ya', 'ttyl', \n",
    "                'wiedersehen'},\n",
    "    'grad': {'grad', 'graduate'},\n",
    "    'hi': {'hello', 'hi', 'hey', 'hiya', 'hola', 'greetings', 'yo'},\n",
    "    'i': {'i', 'myself'},\n",
    "    'long': {'long', 'lengthy'},\n",
    "    'prerequisites': {'prereqs', 'prerequisites'},\n",
    "    'professor': {'professor', 'teacher', 'instructor', 'prof'}\n",
    "}\n",
    "\n",
    "def synonymReplacer(tokens):\n",
    "    # loop through each token in the list\n",
    "    for i, word in enumerate(tokens):\n",
    "        # convert the word to lowercase\n",
    "        word = word.lower()\n",
    "        # loop through each synonym list in the synonyms dictionary\n",
    "        for syn, syn_list in synonyms.items():\n",
    "            # check if the current word is in the synonym list\n",
    "            if word in syn_list:\n",
    "                # replace the current token with the synonym\n",
    "                tokens[i] = syn\n",
    "                # break out of the loop since we've found a synonym\n",
    "                break\n",
    "    # return the modified list of tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7bb7f140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when', 'does', 'course', 'start']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonymReplacer([\"when\", \"does\", \"class\", \"begin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c9b38b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(sentence):\n",
    "    \"\"\"\n",
    "    This function takes a sentence as input and performs various text preprocessing steps on it,\n",
    "    including removing punctuation, stop words, and stemming each word in the sentence.\n",
    "    \"\"\"\n",
    "    # remove punctuation from sentence\n",
    "    sentence = ''.join(\n",
    "        char for char in sentence if char not in string.punctuation\n",
    "    )\n",
    "    # tokenizing the sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # replace synonyms\n",
    "    tokens = synonymReplacer(tokens)\n",
    "    # removing stop words\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    # stemming each word in the sentence\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    # return the preprocessed sentence as a list of words\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "\n",
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    Create a bag of words representation for a given tokenized sentence.\n",
    "    \"\"\"\n",
    "    # initialize the bag with zeros for each word in the vocabulary\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "\n",
    "    # update the bag with 1 for each word in the sentence that exists in the vocabulary\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in tokenized_sentence: \n",
    "            bag[idx] = 1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1242c",
   "metadata": {},
   "source": [
    "#### Preprocessing Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c766d",
   "metadata": {},
   "source": [
    "What exactly does our preprocessing do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a497c33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who', 'lead', 'lectur', 'cours']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"Can you tell me who will be leading the lectures for CS 472?\"\n",
    "tokens = preprocess(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f296c5d",
   "metadata": {},
   "source": [
    "First, the `preprocess` function removes all punctuation marks from the sentence using the `string.punctuation` module. Then, the sentence is tokenized into a list of words using the nltk.word_tokenize method.\n",
    "\n",
    "Next, the function removes stop words, which are common words that do not carry much meaning in the sentence, such as \"a\", \"an\", \"the\", \"of\", and so on. In this case, the function is using a pre-defined list of stop words to remove them from the list of tokens.\n",
    "\n",
    "After that, the function performs stemming on each word in the sentence, which involves converting the words into their root or base form, called their stem. The function uses a stemmer to perform this task.\n",
    "\n",
    "Finally, the preprocessed words are returned as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67a32bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 1., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_all_words = ['hello', 'tell', 'what', 'ten', 'lead', 'lectur', 'cs', 'who', '472']\n",
    "bag_of_words(tokens, example_all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d089fcb",
   "metadata": {},
   "source": [
    "The purpose of the code in the below cell is to read in the data file, tokenize the sentences into individual words, and create a list of (X, y) pairs, where X is a list of tokenized words and y is the associated intent tag. This is a common preprocessing step in Natural Language Processing (NLP) where the goal is to classify user input into one of several predefined categories. By tokenizing the input patterns and creating (X, y) pairs, the data can be transformed into a format that is more suitable for use in machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "37011752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data file as a Python object\n",
    "with open('intents.json', 'r') as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# empty lists for storing the tokenized words, tags, and (X, y) pairs\n",
    "all_words = []\n",
    "tags = []\n",
    "xy_pairs = []\n",
    "\n",
    "# loop through each intent in the data file\n",
    "for intent in intents['intents']:\n",
    "    # get the tag\n",
    "    tag = intent['tag']\n",
    "    # add the tag to our list of tags\n",
    "    tags.append(tag)\n",
    "    # loop through each pattern (sentence) in the intent\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize the pattern into a list of words\n",
    "        words = preprocess(pattern)        \n",
    "        # add the tokenized words to our list of all words\n",
    "        all_words.extend(words)\n",
    "        # add the (words, tag) pair to our list of (X, y) pairs\n",
    "        xy_pairs.append((words, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa11cf9",
   "metadata": {},
   "source": [
    "The purpose of the code is to preprocess a list of words and associated intent tags for use in Natural Language Processing (NLP) tasks such as intent classification. The code first stems and lowercases each word in the list, ignoring any words in a specified ignore list. It then removes duplicate words from the list and sorts the resulting list. The code also sorts the list of intent tags. The resulting processed data can be used as input to machine learning algorithms for tasks such as training a model to classify user input into one of several predefined categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "56014206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate words and sort the list\n",
    "all_words = sorted(set(all_words))\n",
    "\n",
    "# sort the list of tags\n",
    "tags = sorted(set(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219782c5",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661f3b4",
   "metadata": {},
   "source": [
    "The code in the cell below creates the training data by converting each input sentence in the original data to a bag of words representation using the bag_of_words function, and then converting the intent tags to integer labels that can be used for training. The resulting training data is represented as two numpy arrays, X_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5333d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# for each (X, y) pair in the data\n",
    "for (pattern_sentence, tag) in xy_pairs:\n",
    "    # create a bag of words for the pattern sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    # append the bag of words to the X training data\n",
    "    X_train.append(bag)\n",
    "    # convert the tag to a label that can be used with PyTorch CrossEntropyLoss\n",
    "    label = tags.index(tag)\n",
    "    # append the label to the y training data\n",
    "    y_train.append(label)\n",
    "\n",
    "# convert X and y to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18e965",
   "metadata": {},
   "source": [
    "### Create model to classify text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b0af5",
   "metadata": {},
   "source": [
    "In order to effectively capture the underlying patterns in language data, a model capable of learning complex relationships is required. However, it is important to strike a balance and avoid overfitting by not making the model more complex than necessary. To start with, a simple feed forward neural network was chosen.\n",
    "\n",
    "The model presented in the cell below uses three fully connected layers, providing the capability to learn complex patterns in the input data. The ReLU activation function between the first and second fully connected layers introduces non-linearity into the model, improving its ability to model complex relationships. The third fully connected layer maps the learned features to the output classes, and no activation function is used for this layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ab67f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # the first fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # the second fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) \n",
    "        # the third fully connected layer\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        # the ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # apply the ReLU activation function to the first fully connected layer\n",
    "        out = self.relu(self.fc1(x))\n",
    "        # apply the ReLU activation function to the second fully connected layer\n",
    "        out = self.relu(self.fc2(out))\n",
    "        # apply the third fully connected layer without any activation function\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f67312",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "18fb6c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418 16\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "num_epochs = 500\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 16\n",
    "output_size = len(tags)\n",
    "print(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c668e5",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821aed6",
   "metadata": {},
   "source": [
    "The `ChatDataset` class is a PyTorch dataset object that is designed to be used with PyTorch's DataLoader module to retrieve training data and corresponding labels for my chatbot application. The purpose of this class is to encapsulate the training data and labels as attributes of the object and to define the methods `__getitem__()` and `__len__()` to support indexing and length operations, respectively, on the dataset object. This class is an implementation of the Dataset abstract class in PyTorch and provides a consistent interface for loading training data for use with PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3415550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset object that is designed to be used with DataLoader to retrieve\n",
    "    training data and labels.\n",
    "\n",
    "    Attributes:\n",
    "    X_train: numpy array, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    y_train: numpy array, shape (n_samples, n_classes)\n",
    "        Corresponding labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Initializes the ChatDataset object.\n",
    "\n",
    "        Parameters:\n",
    "        X_train: numpy array, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y_train: numpy array, shape (n_samples, n_classes)\n",
    "            Corresponding labels.\n",
    "        \"\"\"\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns the specified training sample.\n",
    "\n",
    "        Parameters:\n",
    "        index: int\n",
    "            Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        Tuple containing the training data and corresponding label for the specified index.\n",
    "        \"\"\"\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of training samples.\n",
    "\n",
    "        Returns:\n",
    "        The number of training samples as an integer.\n",
    "        \"\"\"\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7268998",
   "metadata": {},
   "source": [
    "This code creates a `ChatDataset` object with training data and corresponding labels `X_train` and `y_train`, and then uses a PyTorch DataLoader object to iterate over the dataset during model training. The `DataLoader` is configured to retrieve data in batches of size `batch_size`, to shuffle the data before each epoch of training, and to use 0 worker processes for data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9ec14acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChatDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d91d43",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ac864",
   "metadata": {},
   "source": [
    "First, let's see whether the computer has an GPU available for use with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8f06368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5bfd3f",
   "metadata": {},
   "source": [
    "Now let's instantiate our model, loss function, and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7764f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2aad3",
   "metadata": {},
   "source": [
    "Now we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b9b46a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500], Loss: 0.0001\n",
      "Epoch [200/500], Loss: 0.0000\n",
      "Epoch [300/500], Loss: 0.0000\n",
      "Epoch [400/500], Loss: 0.0000\n",
      "Epoch [500/500], Loss: 0.0000\n",
      "final loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): # loop over the specified number of epochs\n",
    "    for (words, labels) in train_loader: # iterate over training data in batches\n",
    "        words = words.to(device) # load batch onto GPU\n",
    "        labels = labels.to(dtype=torch.long).to(device) # load batch onto GPU\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(words) # make predictions for given inputs\n",
    "        loss = criterion(outputs, labels) # compare predictions to actual labels\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad() # reset gradients\n",
    "        loss.backward() # compute gradients using backpropagation\n",
    "        optimizer.step() # update the model weights\n",
    "\n",
    "    if (epoch+1) % 100 == 0: # print loss every 100 epochs\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}') # print the final loss after training is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76216b6f",
   "metadata": {},
   "source": [
    "### Let's Chat!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ca78e",
   "metadata": {},
   "source": [
    "In the below code, the user is prompted for input until they type 'quit' to exit. Assuming the user did not type quit, their input is preprocessed, converted to a bag of words, and fed into the model for prediction. If the model predicts an intent with a high probability, a response is generated based on that intent from a set of predefined responses. If the model predicts with low probability, the chatbot responds with an \"I do not understand...\" message. Thus, the chatbot will (in theory) only respond to questions it can understand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ee57b3ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'quit' to exit)\n",
      "You: does the course have accomadations for people with disabilities?\n",
      "Eliot: The goal of the class is to be inclusive and accessible to all students, regardless of any disabilities. If a student experiences barriers to full participation or fair evaluation due to a disability, they should inform the instructor so that accommodations can be made. It is also important to inform the instructor early on in the quarter to ensure that appropriate accommodations can be provided. If there are any other obstacles to full participation, students are encouraged to share their concerns with the instructor.\n",
      "You: where are lectures located?\n",
      "Eliot: Lectures are located in room 166 in Lawrence hall\n",
      "You: can you share the zoom link?\n",
      "Eliot: Live lectures will often be viewable by joining the zoom meeting via the following link: https://uoregon.zoom.us/j/91353372386?pwd=MnBMVUh4VC9lU3A3UlJQUmRLY0RjZz09\n",
      "You: when are lectures over?\n",
      "Eliot: Two lectures are delivered in person every week on Tuesdays and Thursdays, from 4:00 PM to 5:20 PM\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "# set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Eliot\"\n",
    "print(\"Let's chat! (type 'quit' to exit)\")\n",
    "while True:\n",
    "    sentence = input(\"You: \")\n",
    "    if sentence.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    sentence = preprocess(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    \n",
    "    if prob.item() > 0.95:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "    else:\n",
    "        print(f\"{bot_name}: I do not understand...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5d808",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- add spell checking to preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9b25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
